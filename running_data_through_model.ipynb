{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model and running reservoir data through the model\n",
    "#Jessica Besnier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Load the trained pipeline\n",
    "loaded_pipeline = joblib.load('Z:/Research_Projects/Reservoir_Project/ML_outputs/models/Run_test_RandomForestRegressor_35_model.joblib')\n",
    "\n",
    "# Prepare new data\n",
    "res_name = 'agua_vermelha'\n",
    "input_data_path = 'Z:/Research_Projects/Reservoir_Project/data/regression_variables/varaibles_ts_clean_top_anom/' + res_name + '_var_normalized.csv'\n",
    "\n",
    "# Load input data\n",
    "input_data = pd.read_csv(input_data_path)\n",
    "input_data = input_data.dropna()\n",
    "\n",
    "def create_regression_dataset():\n",
    "    df = input_data[['normalized_reservoir_height', 'tws', 'precipitation', 'temp', 'volume', 'area', 'discharge', 'Res_time', 'depth', 'wshd_area','elevation' ]] #timeseries and static varaibles for algortihm to tell the difference between reservoirs \n",
    "    \n",
    "    X = df[['tws', 'precipitation', 'temp', 'volume', 'area', 'discharge', 'Res_time', 'depth', 'wshd_area','elevation']]\n",
    "    y = df['normalized_reservoir_height']\n",
    "    return df, X, y\n",
    "\n",
    "def create_datasets():\n",
    "    \"\"\"Create example datasets\"\"\"\n",
    "\n",
    "    df_regression, _, __ = create_regression_dataset()\n",
    "    return df_regression, \n",
    "\n",
    "df_regression = create_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_series_to_tabular(included_columns):\n",
    "    df, _, __ = create_regression_dataset()\n",
    "\n",
    "    TARGET = 'normalized_reservoir_height' # The column in df we want to forecast\n",
    "    LAG = 36 # This is how far back we want to look for features\n",
    "    HORIZON = 6 # This is how far forward we want forecast\n",
    "\n",
    "    # Fill in missing values\n",
    "    cols = df.columns\n",
    "    index = df.index\n",
    "    df = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(df)\n",
    "    df = pd.DataFrame(df, columns=cols, index=index) # convert back to dataframe\n",
    "\n",
    "    df = df[included_columns]    \n",
    "\n",
    "\n",
    "    def create_lag_features(df, target, lag):\n",
    "        \n",
    "        for col in df.columns:\n",
    "            for i in range(1, lag+1):\n",
    "                df[f'{col}-{i}'] = df[col].shift(i)\n",
    "\n",
    "            # Drop non-target values (we only keep historical feature values)\n",
    "            if col != target:\n",
    "                df = df.drop(col, axis=1)\n",
    "        # OPTIONAL: Drop first N rows where N = lag\n",
    "        # Alternatively, we could impute the missing data\n",
    "        df = df.iloc[lag:]\n",
    "        return df \n",
    "\n",
    "    def create_future_values(df, target, horizon):\n",
    "        targets = [ target ]\n",
    "        for i in range(1, horizon):\n",
    "            col_name = f'{target}+{i}'\n",
    "            df[col_name] = df[target].shift(-i)\n",
    "            targets.append(col_name)\n",
    "\n",
    "        # Optional: Drop rows missing future target values\n",
    "        df = df[df[targets[-1]].notna()]\n",
    "        return df, targets\n",
    "\n",
    "\n",
    "    print('\\nInitial df shape:', df.shape)\n",
    "    # Create feature data (X)\n",
    "    df = create_lag_features(df, TARGET, LAG)\n",
    "    print('\\ndf lag shape after feature creation:', df.shape)\n",
    "\n",
    "    # Create targets to forecast (y)\n",
    "    df, targets = create_future_values(df, TARGET, HORIZON)\n",
    "    print('\\ndf horizon shape after feature creation:', df.shape)\n",
    "\n",
    "    # Separate features (X) and targets (y)\n",
    "    y = df[targets]\n",
    "    X = df.drop(targets, axis=1)\n",
    "    #df.set_index('time', inplace=True)\n",
    "\n",
    "    print('\\nShape of X (features):', X.shape)\n",
    "    print('Shape of y (target(s)):', y.shape)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial df shape: (167, 4)\n",
      "\n",
      "df lag shape after feature creation: (131, 145)\n",
      "\n",
      "df horizon shape after feature creation: (126, 150)\n",
      "\n",
      "Shape of X (features): (126, 144)\n",
      "Shape of y (target(s)): (126, 6)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    time_series_to_tabular(included_columns= ['normalized_reservoir_height', 'tws', 'precipitation', 'temp',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial df shape: (167, 4)\n",
      "\n",
      "df lag shape after feature creation: (131, 145)\n",
      "\n",
      "df horizon shape after feature creation: (126, 150)\n",
      "\n",
      "Shape of X (features): (126, 144)\n",
      "Shape of y (target(s)): (126, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\base.py:444: UserWarning: X has feature names, but Normalizer was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 144 features, but Normalizer is expecting 35 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7456\\1368204167.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# #through pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scaler'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX_new_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpredictions_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new_normalized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1952\u001b[0m         \"\"\"\n\u001b[0;32m   1953\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1954\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1955\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m             raise ValueError(\n\u001b[1;32m--> 401\u001b[1;33m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 144 features, but Normalizer is expecting 35 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X, y = time_series_to_tabular(included_columns= ['normalized_reservoir_height', 'tws', 'precipitation', 'temp',])\n",
    "# #through pipeline\n",
    "scaler = loaded_pipeline.named_steps['scaler']\n",
    "X_new_normalized = scaler.transform(X)\n",
    "predictions_new = loaded_pipeline.predict(X_new_normalized)\n",
    "\n",
    "#predictions_new = loaded_pipeline.predict(X)\n",
    "#another option\n",
    "# print(loaded_pipeline.named_steps)\n",
    "# multioutput_regressor = loaded_pipeline.named_steps['multioutput']\n",
    "# model_regressor = multioutput_regressor.estimators_[0]  \n",
    "# X_new_transformed = X_new\n",
    "\n",
    "# predictions_new = model_regressor.predict(X_new_transformed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
