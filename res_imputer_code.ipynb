{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file to determine the percentage of missing values and to impute missing values if less than 10% for a column\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Folder path containing the data files\n",
    "folder_path = 'D:/internship 2022/data/regression_variables/variables_ts_clean_data_avail/'\n",
    "\n",
    "# Get the list of file names in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# List to store the imputed DataFrames\n",
    "imputed_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir: grupiara_var\n",
      "Column: height, Missing Percentage: 28.69%\n",
      "--------------------------------------------------\n",
      "Reservoir: ilha_solteria\n",
      "Column: height, Missing Percentage: 26.50%\n",
      "--------------------------------------------------\n",
      "Reservoir: irape_var\n",
      "Column: tws, Missing Percentage: 20.55%\n",
      "Column: volume, Missing Percentage: 100.00%\n",
      "Column: discharge, Missing Percentage: 100.00%\n",
      "Column: Res_time, Missing Percentage: 100.00%\n",
      "Column: depth, Missing Percentage: 100.00%\n",
      "Column: wshd_area, Missing Percentage: 100.00%\n",
      "--------------------------------------------------\n",
      "Reservoir: itaipu_var\n",
      "Column: Unnamed: 15, Missing Percentage: 100.00%\n",
      "--------------------------------------------------\n",
      "Reservoir: itumbiara_var\n",
      "Column: discharge, Missing Percentage: 100.00%\n",
      "Column: Res_time, Missing Percentage: 100.00%\n",
      "Column: depth, Missing Percentage: 100.00%\n",
      "Column: wshd_area, Missing Percentage: 100.00%\n",
      "Column: elevation, Missing Percentage: 100.00%\n",
      "--------------------------------------------------\n",
      "Reservoir: sao_simao_var\n",
      "Column: height, Missing Percentage: 40.17%\n",
      "--------------------------------------------------\n",
      "Reservoir: barra_bonita_var\n",
      "Column: tws, Missing Percentage: 22.39%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Iterate through each file in the folder\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    reservoir_name = os.path.splitext(file_name)[0]\n",
    "    \n",
    "    # Load the data file into a DataFrame\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_percentage = data.isnull().sum() / len(data) * 100\n",
    "    \n",
    "    # Filter columns with missing percentage greater than X%\n",
    "    filtered_columns = missing_percentage[missing_percentage > 20]\n",
    "    \n",
    "    # Print the reservoir name and filtered columns with missing percentage\n",
    "    if len(filtered_columns) > 0:\n",
    "        print(f'Reservoir: {reservoir_name}')\n",
    "        for column, percentage in filtered_columns.items():\n",
    "            print(f'Column: {column}, Missing Percentage: {percentage:.2f}%')\n",
    "        \n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_embocaco_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_frunas_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_grupiara_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_iepe_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_iguazu_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_ilha_solteria.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_irape_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_itaipu_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_itumbiara_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_jose_richa_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_jurumirim_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_manso_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_nova_avanhandava_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_nova_ponte_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_sao_simao_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_agua_vermelha_var.csv\n",
      "Combined data (imputed and non-imputed) saved to: D:/internship 2022/data/regression_variables/imputed_variables\\combined_barra_bonita_var.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Load the data file into a DataFrame\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Select the columns for imputation\n",
    "    columns_to_impute = ['height', 'tws']\n",
    "    \n",
    "    # Create an IterativeImputer object\n",
    "    imputer = IterativeImputer()\n",
    "    \n",
    "    # Impute missing values in the selected columns\n",
    "    imputed_values = imputer.fit_transform(data[columns_to_impute])\n",
    "    \n",
    "    # Create a DataFrame with the imputed values\n",
    "    imputed_data = pd.DataFrame(imputed_values, columns=columns_to_impute)\n",
    "    \n",
    "    # Combine the imputed data with the non-imputed columns\n",
    "    non_imputed_columns = [col for col in data.columns if col not in columns_to_impute]\n",
    "    combined_data = pd.concat([imputed_data, data[non_imputed_columns]], axis=1)\n",
    "    \n",
    "    # Create a new file name for the combined data\n",
    "    combined_file_name = f'combined_{file_name}'\n",
    "    new_folder_path = 'D:/internship 2022/data/regression_variables/imputed_variables'\n",
    "    combined_file_path = os.path.join(new_folder_path, combined_file_name)\n",
    "    \n",
    "    # Export the combined data to a new CSV file\n",
    "    combined_data.to_csv(combined_file_path, index=False)\n",
    "    \n",
    "    print(f'Combined data (imputed and non-imputed) saved to: {combined_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Index(...) must be called with a collection of some kind, 9 was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29876\\184331356.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Convert the imputed array back to a DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mimputed_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimputed_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Add the dates column back to the DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m                     \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m                     \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m                 )\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     index, columns = _get_axes(\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_get_axes\u001b[1;34m(N, K, index, columns)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mensure_index\u001b[1;34m(index_like, copy)\u001b[0m\n\u001b[0;32m   6334\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6336\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jess_besnier\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scalar_data_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__array__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Index(...) must be called with a collection of some kind, 9 was passed"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loop through each file in the folder and impute missing values\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Store the dates column separately\n",
    "    dates = data['date']\n",
    "    \n",
    "    # Remove the dates column from the DataFrame\n",
    "    data = data.drop('date', axis=1)\n",
    "    \n",
    "    # Create an IterativeImputer object\n",
    "    imputer = IterativeImputer()\n",
    "    \n",
    "    # Impute missing values in the data\n",
    "    imputed_array = imputer.fit_transform(data)\n",
    "    \n",
    "    # Convert the imputed array back to a DataFrame\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=data.columns)\n",
    "    \n",
    "    # Add the dates column back to the DataFrame\n",
    "    imputed_df['date'] = dates\n",
    "    \n",
    "    # Add the imputed DataFrame to the list\n",
    "    imputed_data.append(imputed_df)\n",
    "\n",
    "# Merge the imputed DataFrames on the 'Date' column\n",
    "combined_data = imputed_data[0]  # Start with the first DataFrame\n",
    "for i in range(1, len(imputed_data)):\n",
    "    combined_data = pd.merge(combined_data, imputed_data[i], on='date')\n",
    "\n",
    "# Optional: Sort the DataFrame by the 'Date' column\n",
    "combined_data = combined_data.sort_values(by='date').reset_index(drop=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
